name: Run web scraper

on: 
  schedule:
    - cron: '0 16 * * *' # Run the workflow every day at 18:00 UTC+2

jobs:
  build:
    runs-on: windows-latest
    steps:

      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
          
      - name: Install python packages
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Execute price scrape
        run: python prices.py

      - name: Commit changes
        run: |
          # Stage all CSV files in the Data folder
          git add Price_data/*.csv  # This will include all CSV files in the "Data" folder

          # Commit
          git commit -m "Add new scraped data CSV files"

          # Push the changes back to the repository
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # Required to authenticate the commit
      
      - name: Execute contract scrape
        run: python contracts.py

      - name: Commit changes
        run: |          
          # Stage all CSV files in the Data folder
          git add Contract_data/*.csv  # This will include all CSV files in the "Data" folder

          # Commit
          git commit -m "Add new scraped data CSV files"

          # Push the changes back to the repository
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # Required to authenticate the commit
